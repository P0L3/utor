{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Učitavanje podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "primjer = \"Uvod u teorijsko računarstvo je zabavan!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primjer 1 na razini riječi:     ['Uvod', 'u', 'teorijsko', 'računarstvo', 'je', 'zabavan!']\n",
      "Primjer 2 na razini slova:      ['U', 'v', 'o', 'd', ' ', 'u', ' ', 't', 'e', 'o', 'r', 'i', 'j', 's', 'k', 'o', ' ', 'r', 'a', 'č', 'u', 'n', 'a', 'r', 's', 't', 'v', 'o', ' ', 'j', 'e', ' ', 'z', 'a', 'b', 'a', 'v', 'a', 'n', '!']\n",
      "Primjer 3 na razini \"slogova\":  ['Uvo', 'd u', ' te', 'ori', 'jsk', 'o r', 'aču', 'nar', 'stv', 'o j', 'e z', 'aba', 'van', '!']\n"
     ]
    }
   ],
   "source": [
    "# Na razini riječi\n",
    "print(\"Primjer 1 na razini riječi:    \", primjer.split(\" \"))\n",
    "\n",
    "# Na razini slova\n",
    "print(\"Primjer 2 na razini slova:     \", [p for p in primjer])\n",
    "\n",
    "# Na razini \"slogova\"\n",
    "print(\"Primjer 3 na razini \\\"slogova\\\": \", [primjer[i:i+3] for i in range(0, len(primjer), 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizator na razini riječi za veliku količinu teksta daje veliki vokabular -> Svaki oblik pojedine riječi zaseban je token unutar vokabulara; primjerice: \"*o, doing, does, done*\".\n",
    "\n",
    "Tokenizator na razini znakova (slova) uvijek ima jednako velik vokabular, ali svaki pojedini token ne nosi semantičko značenje; primjerice \"**a***lph***a***bet, st***a***in, ll***a***m***a**\"\n",
    "\n",
    "Tokenizator na razini podriječi (\"slogova\") se čini kao dobar odabir!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Započnimo s jednostavnim korpusom tekstova: 3 rečenice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Ovo su šeste vježbe iz Uvoda u teorijsko računarstvo.\",\n",
    "    \"Na ovim vježbama raditi ćete tokenizaciju i jednostavnu analizu teksta.\",\n",
    "    \"Ovo poglavlje raspravlja o WordPiece tokenizatoru.\",\n",
    "    \"Dobrodošli!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S obzirom da rekreiramo WordPiece tokenizator korišten u radu [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) iskoristiti ćemo pripremljenu predtokenizacijsku za navedeni model (BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p0l3/miniforge3/envs/utor/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Izračunajmo frekvenciju svake pojedine riječi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Ovo': 2, 'su': 1, 'šeste': 1, 'vježbe': 1, 'iz': 1, 'Uvoda': 1, 'u': 1, 'teorijsko': 1, 'računarstvo': 1, '.': 3, 'Na': 1, 'ovim': 1, 'vježbama': 1, 'raditi': 1, 'ćete': 1, 'tokenizaciju': 1, 'i': 1, 'jednostavnu': 1, 'analizu': 1, 'teksta': 1, 'poglavlje': 1, 'raspravlja': 1, 'o': 1, 'WordPiece': 1, 'tokenizatoru': 1, 'Dobrodošli': 1, '!': 1})\n",
      "defaultdict(<class 'int'>, {'Ovo': 2, 'su': 1, 'šeste': 1, 'vježbe': 1, 'iz': 1, 'Uvoda': 1, 'u': 1, 'teorijsko': 1, 'računarstvo': 1, '.': 3, 'Na': 1, 'ovim': 1, 'vježbama': 1, 'raditi': 1, 'ćete': 1, 'tokenizaciju': 1, 'i': 1, 'jednostavnu': 1, 'analizu': 1, 'teksta': 1, 'poglavlje': 1, 'raspravlja': 1, 'o': 1, 'WordPiece': 1, 'tokenizatoru': 1, 'Dobrodošli': 1, '!': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(word_freqs)\n",
    "\n",
    "print(defaultdict(\n",
    "    int, word_freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prisjetimo se abecede (alfabeta) koji je jedinstveni skup nastao od svih početnih slova riječi te svih ostalih slova u riječi s prefiksom \"##\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '##P', '##a', '##b', '##c', '##d', '##e', '##g', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##z', '##č', '##š', '##ž', '.', 'D', 'N', 'O', 'U', 'W', 'a', 'i', 'j', 'o', 'p', 'r', 's', 't', 'u', 'v', 'ć', 'š']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Također dodajemo posebne tokene u vokabular, u ovom slučaju `[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Razdvojimo sve riječi s obzirom na početna slova iz vokabulara:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ovo': ['O', '##v', '##o'], 'su': ['s', '##u'], 'šeste': ['š', '##e', '##s', '##t', '##e'], 'vježbe': ['v', '##j', '##e', '##ž', '##b', '##e'], 'iz': ['i', '##z'], 'Uvoda': ['U', '##v', '##o', '##d', '##a'], 'u': ['u'], 'teorijsko': ['t', '##e', '##o', '##r', '##i', '##j', '##s', '##k', '##o'], 'računarstvo': ['r', '##a', '##č', '##u', '##n', '##a', '##r', '##s', '##t', '##v', '##o'], '.': ['.'], 'Na': ['N', '##a'], 'ovim': ['o', '##v', '##i', '##m'], 'vježbama': ['v', '##j', '##e', '##ž', '##b', '##a', '##m', '##a'], 'raditi': ['r', '##a', '##d', '##i', '##t', '##i'], 'ćete': ['ć', '##e', '##t', '##e'], 'tokenizaciju': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##a', '##c', '##i', '##j', '##u'], 'i': ['i'], 'jednostavnu': ['j', '##e', '##d', '##n', '##o', '##s', '##t', '##a', '##v', '##n', '##u'], 'analizu': ['a', '##n', '##a', '##l', '##i', '##z', '##u'], 'teksta': ['t', '##e', '##k', '##s', '##t', '##a'], 'poglavlje': ['p', '##o', '##g', '##l', '##a', '##v', '##l', '##j', '##e'], 'raspravlja': ['r', '##a', '##s', '##p', '##r', '##a', '##v', '##l', '##j', '##a'], 'o': ['o'], 'WordPiece': ['W', '##o', '##r', '##d', '##P', '##i', '##e', '##c', '##e'], 'tokenizatoru': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##a', '##t', '##o', '##r', '##u'], 'Dobrodošli': ['D', '##o', '##b', '##r', '##o', '##d', '##o', '##š', '##l', '##i'], '!': ['!']}\n"
     ]
    }
   ],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napišimo funkciju koja izračunava \"score\" za svaki par tokena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    \n",
    "    letter_freqs = defaultdict(int) # Riječnik koji prati koliko se pojedini element puta pojavi u tekstu\n",
    "    pair_freqs = defaultdict(int) # Prati koliko se učestalo pojedini par pojavljuje u tekstu\n",
    "    \n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pogledajmo \"score\" prvih 5 parova nakon inicijalnog razdvajanja riječi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('O', '##v'): 0.125\n",
      "('##v', '##o'): 0.03333333333333333\n",
      "('s', '##u'): 0.16666666666666666\n",
      "('š', '##e'): 0.06666666666666667\n",
      "('##e', '##s'): 0.011111111111111112\n",
      "('##s', '##t'): 0.09523809523809523\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronađimo par s najboljim \"score\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('##ž', '##b') 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dakle prvo spajanje koje učimo je `(\"##ž\", \"##b\") -> \"##žb\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append(\"##žb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definirajmo funkciju za spajanje parova:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pogledajmo rezultat prvog spajanja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v', '##j', '##e', '##žb', '##e']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(\"##ž\", \"##b\", splits)\n",
    "splits[\"vježbe\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kreirajmo vokabular sa 100 tokena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '!', '##P', '##a', '##b', '##c', '##d', '##e', '##g', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##z', '##č', '##š', '##ž', '.', 'D', 'N', 'O', 'U', 'W', 'a', 'i', 'j', 'o', 'p', 'r', 's', 't', 'u', 'v', 'ć', 'š', '##žb', '##gl', '##šl', '##dP', 'su', '##ču', 'vj', '##čun', 'an', '##lj', '##sp', '##spr', '##rdP', '##br', 'Ov', 'Uv', '##vlj', 'ov', 'iz', '##ju', '##js', '##jsk', '##st', '##stv', '##rstv', '##vn', '##vnu', '##ru', '##zu', '##kst', '##ri', '##rijsk', 'ovi', 'ovim', '##iz', '##niz', '##dn', '##iju', '##ciju', '##li', '##lizu', '##rdPi', '##šli', '##di', '##dit', '##diti', 'Ovo', 'Uvo', 'Uvod', '##orijsk', '##orijsko', '##rstvo']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vj', '##e', '##žb', '##a']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"vježba\"))\n",
    "print(encode_word(\"HOgging\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ovo', 'su', 'vj', '##e', '##žb', '##e', 'iz', 'Uvod', '##a', 'u', 't', '##e', '##orijsko', 'r', '##a', '##čun', '##a', '##rstvo']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(\"Ovo su vježbe iz Uvoda u teorijsko računarstvo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
