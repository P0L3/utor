{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Učitavanje podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "primjer = \"Uvod u teorijsko računarstvo je zabavan!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primjer 1 na razini riječi:     ['Uvod', 'u', 'teorijsko', 'računarstvo', 'je', 'zabavan!']\n",
      "Primjer 2 na razini slova:      ['U', 'v', 'o', 'd', ' ', 'u', ' ', 't', 'e', 'o', 'r', 'i', 'j', 's', 'k', 'o', ' ', 'r', 'a', 'č', 'u', 'n', 'a', 'r', 's', 't', 'v', 'o', ' ', 'j', 'e', ' ', 'z', 'a', 'b', 'a', 'v', 'a', 'n', '!']\n",
      "Primjer 3 na razini \"slogova\":  ['Uvo', 'd u', ' te', 'ori', 'jsk', 'o r', 'aču', 'nar', 'stv', 'o j', 'e z', 'aba', 'van', '!']\n"
     ]
    }
   ],
   "source": [
    "# Na razini riječi\n",
    "print(\"Primjer 1 na razini riječi:    \", primjer.split(\" \"))\n",
    "\n",
    "# Na razini slova\n",
    "print(\"Primjer 2 na razini slova:     \", [p for p in primjer])\n",
    "\n",
    "# Na razini \"slogova\"\n",
    "print(\"Primjer 3 na razini \\\"slogova\\\": \", [primjer[i:i+3] for i in range(0, len(primjer), 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizator na razini riječi za veliku količinu teksta daje veliki vokabular -> Svaki oblik pojedine riječi zaseban je token unutar vokabulara; primjerice: \"*o, doing, does, done*\".\n",
    "\n",
    "Tokenizator na razini znakova (slova) uvijek ima jednako velik vokabular, ali svaki pojedini token ne nosi semantičko značenje; primjerice \"**a***lph***a***bet, st***a***in, ll***a***m***a**\"\n",
    "\n",
    "Tokenizator na razini podriječi (\"slogova\") se čini kao dobar odabir!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Započnimo s jednostavnim korpusom tekstova: 3 rečenice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Ovo su šeste vježbe iz Uvoda u teorijsko računarstvo.\",\n",
    "    \"Na ovim vježbama raditi ćete tokenizaciju i jednostavnu analizu teksta.\",\n",
    "    \"Ovo poglavlje raspravlja o WordPiece tokenizatoru.\",\n",
    "    \"Dobrodošli!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S obzirom da rekreiramo WordPiece tokenizator korišten u radu [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) iskoristiti ćemo pripremljenu predtokenizacijsku za navedeni model (BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p0l3/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Izračunajmo frekvenciju svake pojedine riječi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Ovo': 2, 'su': 1, 'šeste': 1, 'vježbe': 1, 'iz': 1, 'Uvoda': 1, 'u': 1, 'teorijsko': 1, 'računarstvo': 1, '.': 3, 'Na': 1, 'ovim': 1, 'vježbama': 1, 'raditi': 1, 'ćete': 1, 'tokenizaciju': 1, 'i': 1, 'jednostavnu': 1, 'analizu': 1, 'teksta': 1, 'poglavlje': 1, 'raspravlja': 1, 'o': 1, 'WordPiece': 1, 'tokenizatoru': 1, 'Dobrodošli': 1, '!': 1})\n",
      "defaultdict(<class 'int'>, {'Ovo': 2, 'su': 1, 'šeste': 1, 'vježbe': 1, 'iz': 1, 'Uvoda': 1, 'u': 1, 'teorijsko': 1, 'računarstvo': 1, '.': 3, 'Na': 1, 'ovim': 1, 'vježbama': 1, 'raditi': 1, 'ćete': 1, 'tokenizaciju': 1, 'i': 1, 'jednostavnu': 1, 'analizu': 1, 'teksta': 1, 'poglavlje': 1, 'raspravlja': 1, 'o': 1, 'WordPiece': 1, 'tokenizatoru': 1, 'Dobrodošli': 1, '!': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(word_freqs)\n",
    "\n",
    "print(defaultdict(\n",
    "    int, word_freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prisjetimo se abecede (alfabeta) koji je jedinstveni skup nastao od svih početnih slova riječi te svih ostalih slova u riječi s prefiksom \"##\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Zadatak 1: Inicijalizirajte osnovni vokabular (alfabet) za WordPiece tokenizator izdvajanjem svih jedinstvenih znakova iz skupa podataka. Morate razlikovati znakove koji se nalaze na početku riječi od onih koji se pojavljuju unutar riječi.\n",
    "\n",
    "> (*Hint:* Znakovi koji nisu na početku riječi moraju dobiti prefiks `##` kako bi se označilo da su dio sufiksa ili sredine riječi – npr. slovo \"a\" postaje `##a` ako nije prvo slovo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '##P', '##a', '##b', '##c', '##d', '##e', '##g', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##z', '##č', '##š', '##ž', '.', 'D', 'N', 'O', 'U', 'W', 'a', 'i', 'j', 'o', 'p', 'r', 's', 't', 'u', 'v', 'ć', 'š']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Također dodajemo posebne tokene u vokabular, u ovom slučaju `[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Razdvojimo sve riječi s obzirom na početna slova iz vokabulara:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ovo': ['O', '##v', '##o'], 'su': ['s', '##u'], 'šeste': ['š', '##e', '##s', '##t', '##e'], 'vježbe': ['v', '##j', '##e', '##ž', '##b', '##e'], 'iz': ['i', '##z'], 'Uvoda': ['U', '##v', '##o', '##d', '##a'], 'u': ['u'], 'teorijsko': ['t', '##e', '##o', '##r', '##i', '##j', '##s', '##k', '##o'], 'računarstvo': ['r', '##a', '##č', '##u', '##n', '##a', '##r', '##s', '##t', '##v', '##o'], '.': ['.'], 'Na': ['N', '##a'], 'ovim': ['o', '##v', '##i', '##m'], 'vježbama': ['v', '##j', '##e', '##ž', '##b', '##a', '##m', '##a'], 'raditi': ['r', '##a', '##d', '##i', '##t', '##i'], 'ćete': ['ć', '##e', '##t', '##e'], 'tokenizaciju': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##a', '##c', '##i', '##j', '##u'], 'i': ['i'], 'jednostavnu': ['j', '##e', '##d', '##n', '##o', '##s', '##t', '##a', '##v', '##n', '##u'], 'analizu': ['a', '##n', '##a', '##l', '##i', '##z', '##u'], 'teksta': ['t', '##e', '##k', '##s', '##t', '##a'], 'poglavlje': ['p', '##o', '##g', '##l', '##a', '##v', '##l', '##j', '##e'], 'raspravlja': ['r', '##a', '##s', '##p', '##r', '##a', '##v', '##l', '##j', '##a'], 'o': ['o'], 'WordPiece': ['W', '##o', '##r', '##d', '##P', '##i', '##e', '##c', '##e'], 'tokenizatoru': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##a', '##t', '##o', '##r', '##u'], 'Dobrodošli': ['D', '##o', '##b', '##r', '##o', '##d', '##o', '##š', '##l', '##i'], '!': ['!']}\n"
     ]
    }
   ],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Zadatak 2: Napravite funkciju (`compute_pair_score`) koja na temelju riječnika `splits` izračunava `score` pojedinog para na temelju zadane formule: $score = (|(a,b)|) / (|a|*|b|)$.\n",
    "\n",
    "> $(a,b)$ - uzastopno pojavljivanje dva tokena \"a\" i \"b\".\n",
    "\n",
    "> Primjeri:\n",
    ">\n",
    "> `compute_pair_score(\"s\", \"##u\", splits)` output:  `0.16666666666666666`\n",
    ">\n",
    "> `compute_pair_score(\"##t\", \"##i\", splits)` output: `0.014285714285714285`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_score(a, b, splits):\n",
    "\n",
    "    count_a = 0\n",
    "    count_b = 0\n",
    "    count_ab = 0\n",
    "    for word in splits:\n",
    "        # print()\n",
    "        previous = \"\"\n",
    "        for token in splits[word]:\n",
    "            if token == a:\n",
    "                count_a += 1\n",
    "            elif token == b:\n",
    "                count_b += 1\n",
    "                if previous == a:\n",
    "                    count_ab += 1\n",
    "            else:\n",
    "                pass\n",
    "            previous = token\n",
    "    return count_ab / (count_a * count_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014285714285714285"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_pair_score(\"##t\", \"##i\", splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napišimo funkciju koja izračunava \"score\" za svaki par tokena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    \n",
    "    letter_freqs = defaultdict(int) # Riječnik koji prati koliko se pojedini element puta pojavi u tekstu\n",
    "    pair_freqs = defaultdict(int) # Prati koliko se učestalo pojedini par pojavljuje u tekstu\n",
    "    \n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pogledajmo \"score\" prvih 5 parova nakon inicijalnog razdvajanja riječi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('O', '##v'): 0.125\n",
      "('##v', '##o'): 0.03333333333333333\n",
      "('s', '##u'): 0.16666666666666666\n",
      "('š', '##e'): 0.06666666666666667\n",
      "('##e', '##s'): 0.011111111111111112\n",
      "('##s', '##t'): 0.09523809523809523\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronađimo par s najboljim \"score\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('##ž', '##b') 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dakle prvo spajanje koje učimo je `(\"##ž\", \"##b\") -> \"##žb\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append(\"##žb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definirajmo funkciju za spajanje parova:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pogledajmo rezultat prvog spajanja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v', '##j', '##e', '##žb', '##e']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(\"##ž\", \"##b\", splits)\n",
    "splits[\"vježbe\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kreirajmo vokabular sa 100 tokena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '!', '##P', '##a', '##b', '##c', '##d', '##e', '##g', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##z', '##č', '##š', '##ž', '.', 'D', 'N', 'O', 'U', 'W', 'a', 'i', 'j', 'o', 'p', 'r', 's', 't', 'u', 'v', 'ć', 'š', '##žb', '##žb', '##gl', '##šl', '##dP', 'su', '##ču', 'vj', '##čun', 'an', '##lj', '##sp', '##spr', '##rdP', '##br', 'Ov', 'Uv', '##vlj', 'ov', 'iz', '##ju', '##js', '##jsk', '##st', '##stv', '##rstv', '##vn', '##vnu', '##ru', '##zu', '##kst', '##ri', '##rijsk', 'ovi', 'ovim', '##iz', '##niz', '##dn', '##iju', '##ciju', '##li', '##lizu', '##rdPi', '##šli', '##di', '##dit', '##diti', 'Ovo', 'Uvo', 'Uvod', '##orijsk', '##orijsko']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vj', '##e', '##žb', '##a']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"vježba\"))\n",
    "print(encode_word(\"HOgging\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ovo', 'su', 'vj', '##e', '##žb', '##e', 'iz', 'Uvod', '##a', 'u', 't', '##e', '##orijsko', 'r', '##a', '##čun', '##a', '##rstvo', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(\"Ovo su vježbe iz Uvoda u teorijsko računarstvo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
